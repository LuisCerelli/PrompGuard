## ğŸ“ Proyecto: AI-Prompt-Validator  

### ğŸ“Œ **Objetivo del Proyecto**  

El objetivo de este proyecto es diseÃ±ar un **sistema que valide y corrija los prompts antes de enviarlos a la IA**. Esto ayuda a optimizar las respuestas generadas, asegurando que sean **claras, conformes y libres de riesgos potenciales** (por ejemplo, sesgo, lenguaje daÃ±ino o datos sensibles).  

Para lograr esto, configuramos diferentes servicios en **Azure** y creamos una **arquitectura backend** basada en **Azure Functions** y **Azure OpenAI**.  

---

## âš™ï¸ **ConfiguraciÃ³n Paso a Paso**  

### 1ï¸âƒ£ **Crear Azure Function App**  
ğŸ’¡ **FunciÃ³n principal donde correrÃ¡ nuestra validaciÃ³n.**  

1. Ir al **Portal de Azure** [ğŸ”— Azure Portal](https://portal.azure.com).  
2. En la barra de bÃºsqueda, escribir **Function App** Ã³ **Aplicacion de Funcion** si estÃ¡ en espaÃ±ol y hacer clic en **"Crear"**.  

**Nota:** en caso que la barra de <u>Marketplace</u> no lo encuentre usar este link luego de loguearse:  
```
https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites/kind/functionapp
```
3. Configurar los siguientes valores:  
   - **SuscripciÃ³n:** Seleccionar la suscripciÃ³n activa.  
   - **Grupo de recursos:** **AI-Prompt-Validator-RG** (si no existe, crearlo).  
   - **Nombre de la Function App:** **prompt-validator-function**.  
   - **RegiÃ³n:** **East US 2**.  
   - **Pila de tiempo de ejecuciÃ³n:** **Python 3.10**.  
   - **TamaÃ±o de instancia:** **2048 MB** (para optimizar costos).  
4. En la pestaÃ±a **Monitoreo**, seleccionar:  
   - **Habilitar Application Insights:** **SÃ­**.  
   - **Application Insights Resource:** **prompt-validator-function-openai-b083** (creado automÃ¡ticamente).  
5. Hacer clic en **"Revisar y Crear"** â†’ **"Crear"** y esperar la implementaciÃ³n.  

---

### 2ï¸âƒ£ **Habilitar Azure OpenAI**  
ğŸ’¡ **Este servicio procesarÃ¡ las correcciones y validaciones de los prompts.**  

1. En el Portal de Azure, buscar **"Azure OpenAI"**.  
2. Hacer clic en **"Habilitar Azure OpenAI"**.  
3. Configurar los siguientes valores:  
   - **RegiÃ³n:** **East US 2** (misma que Function App).  
   - **Recurso creado:** **prompt-validator-function-openai-b083**.  
   - **Almacenamiento vectorial:** Seleccionar **"Configurar mÃ¡s tarde"**.  
4. Hacer clic en **"Revisar y Crear"** â†’ **"Crear"** y esperar la implementaciÃ³n.  

---

### 3ï¸âƒ£ **Configurar Azure Cognitive Services - Language Service**  
ğŸ’¡ **Este servicio nos permitirÃ¡ corregir la gramÃ¡tica y optimizar el texto.**  

1. En el Portal de Azure, buscar **"Language Service"**.  
2. Hacer clic en **"Crear"**.  
3. Configurar los siguientes valores:  
   - **SuscripciÃ³n:** Seleccionar la suscripciÃ³n activa.  
   - **Grupo de recursos:** **AI-Prompt-Validator-RG**.  
   - **Nombre del servicio:** **prompt-language-service**.  
   - **RegiÃ³n:** **East US 2**.  
   - **Plan de tarifa:** **S (Standard, 1K llamadas/minuto)**.  
4. Hacer clic en **"Revisar y Crear"** â†’ **"Crear"** y esperar la implementaciÃ³n.  

---

### 4ï¸âƒ£ **Obtener las Claves y Endpoint de los Servicios**  
ğŸ’¡ **Necesitamos estas credenciales para integrarlas en nuestro cÃ³digo.**  

ğŸ“Œ **Para Azure OpenAI:**  
1. Ir a **Azure OpenAI** â†’ **prompt-validator-function-openai-b083**.  
2. En el menÃº lateral, hacer clic en **"Keys and Endpoint"**.  
3. Copiar:  
   - **Endpoint:** `https://turecurso.openai.azure.com/`  
   - **Key:** `tu-clave-de-openai`  

ğŸ“Œ **Para Azure Cognitive Services - Language Service:**  
1. Ir a **Language Service** â†’ **prompt-language-service**.  
2. En el menÃº lateral, hacer clic en **"Keys and Endpoint"**.  
3. Copiar:  
   - **Endpoint:** `https://turecurso.cognitiveservices.azure.com/`  
   - **Key:** `tu-clave-de-cognitive-services`  

---

### 5ï¸âƒ£ **Actualizar el archivo `local.settings.json` en Azure Functions**  
ğŸ’¡ **Para que nuestra funciÃ³n use las credenciales correctas.**  

1. Abrir el proyecto en **Visual Studio Code**.  
2. Dentro de la carpeta de Azure Functions, localizar **`local.settings.json`**.  
3. Reemplazar el contenido con las credenciales copiadas:  

```json
{
  "IsEncrypted": false,
  "Values": {
   ...
    "CONTENT_SAFETY_ENDPOINT": "https://turecurso.cognitiveservices.azure.com/",
    "CONTENT_SAFETY_KEY": "tu-clave-de-content-safety"
   ...
  }
}
```

DespuÃ©s de haber configurado los servicios base en Azure, ahora hemos **implementado los modelos en Azure OpenAI** y actualizado nuestra funciÃ³n en Azure Functions para conectarse correctamente con estos recursos.  

---

## âš™ï¸ **5ï¸âƒ£ ImplementaciÃ³n de Modelos en Azure OpenAI**  

ğŸ’¡ Para validar y generar respuestas optimizadas, implementamos **GPT-35-Turbo** en **Azure AI Foundry** con dos configuraciones:  

1. **CorrecciÃ³n de Prompts** â†’ `corrector-deployment`.  
2. **GeneraciÃ³n de Respuestas** â†’ `respuesta-deployment`.  

ğŸ“Œ **Pasos seguidos:**  

1. Entramos en **Azure AI Foundry** desde el **Portal de Azure**.  
2. En la pestaÃ±a **Implementaciones**, hicimos clic en **"Nueva implementaciÃ³n"**.  
3. **Configuramos cada implementaciÃ³n** con:  
   - **Modelo:** `gpt-35-turbo`.  
   - **Tipo de implementaciÃ³n:** `EstÃ¡ndar`.  
   - **VersiÃ³n del modelo:** `0125` (Ãºltima estable).  
   - **Filtro de contenido:** `DefaultV2`.  
   - **Cuota dinÃ¡mica:** **Activada**.  
   - **Tokens por minuto (TPM):** **Reducido a 10K** (para evitar problemas de cuota).  
4. Esperamos la activaciÃ³n y verificamos que ambas implementaciones aparezcan en la lista de **Implementaciones Activas**.  

---

## ğŸ”‘ **6ï¸âƒ£ ObtenciÃ³n de Credenciales de Azure OpenAI**  

DespuÃ©s de implementar los modelos, **obtenemos las claves y el endpoint** para integrarlos en nuestro backend.  

ğŸ“Œ **Pasos seguidos:**  

1. En el **Portal de Azure**, buscamos el recurso **`prompt-validator-function-openai-b083`**.  
2. Hicimos clic en **"Click here to view endpoints"** y copiamos la URL del endpoint.  
3. Hicimos clic en **"Click here to manage keys"** y copiamos la **Key 1**.  

ğŸ“Œ **Ejemplo de credenciales obtenidas:**  
- **Endpoint:** `https://turecurso.openai.azure.com/`  
- **Key:** `tu-clave-de-openai`  

---

## ğŸ›  **7ï¸âƒ£ ConfiguraciÃ³n de Azure Functions (`local.settings.json`)**  

Ahora, actualizamos **Azure Functions** para conectar correctamente OpenAI con nuestra API.  

ğŸ“Œ **Pasos seguidos:**  

1. Abrimos el proyecto en **Visual Studio Code**.  
2. Modificamos el archivo **`local.settings.json`** para incluir las claves y endpoints de OpenAI.  

ğŸ“Œ **Ejemplo de `local.settings.json` actualizado:**  

```json
{
  "IsEncrypted": false,
  "Values": {
   ...
    "OPENAI_ENDPOINT": "https://turecurso.openai.azure.com/",#-> AquÃ­ colocamos el Endpoint
    "OPENAI_KEY": "tu-clave-de-openai",#-> AquÃ­ colocamos la KEY 1
    "OPENAI_CORRECTOR_DEPLOYMENT": "corrector-deployment",
    "OPENAI_RESPUESTA_DEPLOYMENT": "respuesta-deployment",
   ...
  }
}
```

## ğŸ“Œ **8ï¸âƒ£ VerificaciÃ³n en Azure Functions**  

Antes de continuar con el desarrollo del backend, verificamos que **Azure Functions estÃ© correctamente configurado**.  

ğŸ“Œ **Pasos seguidos:**  

1. Entramos a **Azure Functions** en el **Portal de Azure**.  
2. Buscamos nuestra funciÃ³n **`prompt-validator-function`**.  
3. En el menÃº lateral, fuimos a **ConfiguraciÃ³n > Variables de AplicaciÃ³n**.  
4. **Verificamos que las claves y endpoints de OpenAI y Cognitive Services estÃ©n correctamente cargados.**  

ğŸš€ **Con esto, Azure Functions ya estÃ¡ listo para conectarse con OpenAI.**  


## ğŸ›  **9ï¸âƒ£ ImplementaciÃ³n de los Endpoints en Azure Functions**  

DespuÃ©s de configurar correctamente OpenAI en Azure, implementamos los endpoints para **validaciÃ³n de prompts** y **generaciÃ³n de respuestas** en nuestra funciÃ³n de Azure.  

ğŸ“Œ **Endpoints creados:**  
- `POST /api/validatePrompt` â†’ Valida el contenido del prompt asegurando seguridad y claridad.  
- `POST /api/generateResponse` â†’ Genera respuestas usando **GPT-4** en Azure OpenAI.  

### ğŸ“‚ **Estructura del Proyecto**  

DespuÃ©s de la implementaciÃ³n, nuestro proyecto tiene la siguiente estructura:  

```bash
myFunctionApp
â”œâ”€â”€ PromtGuard
â”‚   â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ pyvenv.cfg
â”œâ”€â”€ __pycache__/
â”œâ”€â”€ function_app.py
â”œâ”€â”€ function_app.zip
â”œâ”€â”€ generateResponse/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ function.json
â”œâ”€â”€ host.json
â”œâ”€â”€ local.settings.json
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ validatePrompt/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ function.json
```

---

## ğŸš€ **ğŸ”Ÿ EjecuciÃ³n de la API Localmente**  

Una vez que todo estÃ¡ configurado, podemos **ejecutar la API localmente** usando **Azure Functions Core Tools**.  

### âœ… **Pasos para Iniciar la API**  

1ï¸âƒ£ **Abrir la terminal y navegar al proyecto:**  
```sh
cd myFunctionApp
```

2ï¸âƒ£ **Activar el entorno virtual:**  
```sh
source PromtGuard/bin/activate  # Mac/Linux
```

3ï¸âƒ£ **Ejecutar Azure Functions:**  
```sh
func start
```

ğŸ“Œ **Ejemplo de salida esperada:**  
```bash
Azure Functions Core Tools
Function Runtime Version: 4.1036.1.23224

Functions:

        generateResponse: [POST] http://localhost:7071/api/generateResponse
        validatePrompt: [POST] http://localhost:7071/api/validatePrompt
```

Si ves estas rutas en la terminal, **las funciones estÃ¡n activas y listas para recibir solicitudes**. ğŸ‰  

---

## ğŸ§ª **1ï¸âƒ£1ï¸âƒ£ Pruebas de los Endpoints**  

ğŸ“Œ Para probar que la API funciona correctamente, enviamos solicitudes con `curl` o Postman.  

### ğŸ”¹ **ValidaciÃ³n de Prompt** (`/api/validatePrompt`)  

```sh
curl -X POST "http://localhost:7071/api/validatePrompt" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Hola, Â¿cÃ³mo estÃ¡s?"}'
```

ğŸ”¹ **Ejemplo de respuesta esperada:**  
```json
{
  "correctedPrompt": "Hola, Â¿cÃ³mo estÃ¡s?",
  "suggestions": ["Reformula la pregunta para mayor claridad."]
}
```

---

### ğŸ”¹ **GeneraciÃ³n de Respuesta** (`/api/generateResponse`)  

```sh
curl -X POST "http://localhost:7071/api/generateResponse" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Escribe un resumen sobre el cambio climÃ¡tico."}'
```

ğŸ”¹ **Ejemplo de respuesta esperada:**  
```json
{
  "response": "El cambio climÃ¡tico es un fenÃ³meno global causado por la emisiÃ³n de gases de efecto invernadero..."
}
```

---

## ğŸš‘ **1ï¸âƒ£2ï¸âƒ£ SoluciÃ³n de Problemas**  

Si la API no responde o tienes errores, revisa lo siguiente:  

âŒ **Error:** `Worker failed to index functions`  
ğŸ”¹ **SoluciÃ³n:** AsegÃºrate de que `app` estÃ¡ correctamente definido en `function_app.py`.  

âŒ **Error:** `No job functions found`  
ğŸ”¹ **SoluciÃ³n:** Verifica que cada funciÃ³n tiene correctamente el decorador `@app.function_name()`.  

âŒ **Error 500 al llamar a OpenAI**  
ğŸ”¹ **SoluciÃ³n:** Revisa tu clave de OpenAI en `local.settings.json` y confirma que estÃ¡ activa.  

---

## ğŸ¯ **1ï¸âƒ£3ï¸âƒ£ PrÃ³ximos Pasos**  

âœ… **Mejorar la validaciÃ³n del contenido para reducir falsos positivos.**  
âœ… **Optimizar la integraciÃ³n con OpenAI para mejorar respuestas.**  
âœ… **Desplegar en Azure Function App y probar en producciÃ³n.**  





